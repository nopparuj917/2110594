{"cells":[{"cell_type":"markdown","metadata":{"id":"2YprqbOPMc5a"},"source":["## Import Libs"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6540,"status":"ok","timestamp":1677418293698,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"heICP79cMc5e"},"outputs":[],"source":["%matplotlib inline\n","import time\n","import pandas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import pandas as pd \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import pythainlp\n","import csv\n","from torch.utils.data import Dataset, DataLoader\n","from IPython.display import display\n","from pythainlp.tokenize import word_tokenize\n","from collections import defaultdict\n","from torchtext.vocab import build_vocab_from_iterator\n","from pythainlp.tokenize import word_tokenize\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","import re\n","from sklearn.model_selection import train_test_split\n","import torchmetrics"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import torch\n","torch.manual_seed(0)\n","\n","\n","import random\n","random.seed(0)\n","\n","import numpy as np\n","np.random.seed(0)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["datapath = \"/home/nopparuj/CP3-S2/NLP/takehome/\"\n","df_occupation = pd.read_csv(datapath+\"occupation_mapper.csv\")\n","df_test = pd.read_csv(datapath+\"test_for_submission.csv\", index_col=\"Id\")\n","df_train = pd.read_csv(datapath+\"train.csv\")\n","df_train = df_train.drop(\n","    [\"occupation_group\", \"occupation\", \"occupation_group_index\"], axis=1)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Data cleaning\n","df_train.job_title = df_train.job_title.apply(lambda x : x.lower().strip())\n","df_test.job_title = df_test.job_title.apply(lambda x : x.lower().strip())\n","\n","def clean_html(x):\n","    r = re.compile('<.*?>') \n","    return re.sub(r, ' ', x)\n","\n","def clean_special_chars(text):\n","    text = re.sub(r'[^\\w\\s\\u0E00-\\u0E7F]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text\n","\n","\n","df_train.job_title = df_train.job_title.apply(clean_html)\n","df_test.job_title = df_test.job_title.apply(clean_html)\n","df_train.job_title = df_train.job_title.apply(clean_special_chars)\n","df_test.job_title = df_test.job_title.apply(clean_special_chars)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# tokenized\n","train_title = []\n","train_data = []\n","test_data = []\n","for i in df_train.iterrows():\n","    row = i[1]\n","    title = row[\"job_title\"]\n","    title = word_tokenize(title, engine=\"newmm\")\n","    train_title.append((title))\n","    train_data.append([row[\"occupation_index\"],title])\n","\n","vocab = build_vocab_from_iterator(train_title, specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])\n","\n","encoding_data = [[label,vocab(tokens)] for label,tokens in train_data]\n","for i in df_test.iterrows():\n","    row = i[1]\n","    title = row[\"job_title\"]\n","    title = word_tokenize(title)\n","    test_data.append([0,vocab(title)])"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677418330000,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"ZFbWhiMF-7oL"},"outputs":[],"source":["class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        dropout = self.dropout(embedded)\n","        return self.fc(dropout)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["tacc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(df_occupation))\n","tf1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=len(df_occupation))"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1677418333727,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"WrjciLUXBfc6"},"outputs":[],"source":["def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","         label_list.append(_label)\n","         processed_text = torch.tensor(_text, dtype=torch.int64)\n","         text_list.append(processed_text)\n","         offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677418454210,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"fzXiyzXugeCS"},"outputs":[],"source":["batch_size = 64\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_class = len(df_occupation)\n","emsize = 300\n","model = TextClassificationModel(len(vocab), emsize, num_class).to(device)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["train_data, val_data = train_test_split(encoding_data, test_size=0.01, shuffle=True)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,collate_fn=collate_batch)\n","val_dataloader = DataLoader(val_data, batch_size=1, shuffle=False,collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False,collate_fn=collate_batch)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28730,"status":"ok","timestamp":1677418369508,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"RR_0L8ZVhp7T","outputId":"498cabaa-b60b-4532-bbfa-80e33414c3c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 | train loss: 6.6980 | acc: 0.0125 | f1: 0.0125 \n","Epoch 2 | train loss: 5.9389 | acc: 0.0875 | f1: 0.0875 \n","Epoch 3 | train loss: 5.2884 | acc: 0.1375 | f1: 0.1375 \n","Epoch 4 | train loss: 4.7045 | acc: 0.1500 | f1: 0.1500 \n","Epoch 5 | train loss: 4.1757 | acc: 0.1875 | f1: 0.1875 \n","Epoch 6 | train loss: 3.7183 | acc: 0.2500 | f1: 0.2500 \n","Epoch 7 | train loss: 3.3197 | acc: 0.3125 | f1: 0.3125 \n","Epoch 8 | train loss: 2.9660 | acc: 0.3750 | f1: 0.3750 \n","Epoch 9 | train loss: 2.6746 | acc: 0.4000 | f1: 0.4000 \n","Epoch 10 | train loss: 2.3968 | acc: 0.4500 | f1: 0.4500 \n","Epoch 11 | train loss: 2.1614 | acc: 0.5125 | f1: 0.5125 \n","Epoch 12 | train loss: 1.9468 | acc: 0.5375 | f1: 0.5375 \n","Epoch 13 | train loss: 1.7546 | acc: 0.5375 | f1: 0.5375 \n","Epoch 14 | train loss: 1.6005 | acc: 0.5500 | f1: 0.5500 \n","Epoch 15 | train loss: 1.4443 | acc: 0.5875 | f1: 0.5875 \n","Epoch 16 | train loss: 1.3116 | acc: 0.6250 | f1: 0.6250 \n","Epoch 17 | train loss: 1.2030 | acc: 0.6250 | f1: 0.6250 \n","Epoch 18 | train loss: 1.1010 | acc: 0.6250 | f1: 0.6250 \n","Epoch 19 | train loss: 1.0157 | acc: 0.6375 | f1: 0.6375 \n","Epoch 20 | train loss: 0.9268 | acc: 0.6250 | f1: 0.6250 \n","Epoch 21 | train loss: 0.8520 | acc: 0.6375 | f1: 0.6375 \n","Epoch 22 | train loss: 0.7962 | acc: 0.6625 | f1: 0.6625 \n","Epoch 23 | train loss: 0.7310 | acc: 0.6750 | f1: 0.6750 \n","Epoch 24 | train loss: 0.6839 | acc: 0.6875 | f1: 0.6875 \n","Epoch 25 | train loss: 0.6309 | acc: 0.7000 | f1: 0.7000 \n","Epoch 26 | train loss: 0.5908 | acc: 0.7000 | f1: 0.7000 \n","Epoch 27 | train loss: 0.5498 | acc: 0.6875 | f1: 0.6875 \n","Epoch 28 | train loss: 0.5129 | acc: 0.6875 | f1: 0.6875 \n","Epoch 29 | train loss: 0.4775 | acc: 0.6750 | f1: 0.6750 \n","Epoch 30 | train loss: 0.4512 | acc: 0.6875 | f1: 0.6875 \n"]}],"source":["EPOCHS = 30\n","LR = 0.001\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    model.train()\n","    cnt, loss_sum = 0, 0\n","\n","    for idx, (label, text, offsets) in enumerate(train_dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label).mean()\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        loss_sum += loss.item()\n","        optimizer.step()\n","        cnt += 1\n","    loss_train = loss_sum / cnt\n","    \n","    model.eval()\n","    preds = []\n","    true = []\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(val_dataloader):\n","            predicted_label = model(text, offsets)\n","            preds.append(predicted_label.argmax(dim=1).cpu().tolist())\n","            true.append(label.cpu().tolist())\n","\n","    preds = torch.Tensor(preds).flatten()\n","    true = torch.Tensor(true).flatten()\n","    acc_val =  tacc(preds, true)\n","    f1_val = tf1(preds, true)\n","\n","    print(f\"Epoch {epoch} | train loss: {loss_train:.04f} | acc: {acc_val:.04f} | f1: {f1_val:.04f} \")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":841,"status":"ok","timestamp":1677418390193,"user":{"displayName":"Nopparuj Poonsubanan","userId":"17746876236445804675"},"user_tz":-420},"id":"I4eM-ZeySJVb","outputId":"4480fc38-9266-44f2-b4b1-a76507760500"},"outputs":[],"source":["model.eval()\n","\n","import csv\n","\n","with torch.no_grad(), open('predictions.csv', 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['Id', 'Predicted'])  # Write header row\n","    for idx, (label, text, offsets) in enumerate(test_dataloader):\n","        predicted_label = model(text, offsets)\n","        pre = predicted_label.argmax(dim=1).item()\n","        writer.writerow([idx, pre])  # Write prediction row\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1pOHh2mGySu-UnXJ1iYAHoeFjpRox_XZP","timestamp":1677343546259}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"262229d0ff31be4237519a2fdb648597f2e39a62eae7482efd76cd48d4fcb075"}}},"nbformat":4,"nbformat_minor":0}
